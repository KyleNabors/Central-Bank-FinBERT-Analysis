{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "# Find and import config file\n",
    "config_path = os.getcwd()\n",
    "\n",
    "sys.path.append(config_path)\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = config.database\n",
    "central_banks = config.central_banks\n",
    "training_data = os.path.join(database, \"Training Data\")\n",
    "fed_docs = config.fed_docs\n",
    "ecb_docs = config.ecb_docs\n",
    "boe_docs = config.boe_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_map = pd.read_csv(os.path.join(cwd, \"url_map.csv\"))\n",
    "\n",
    "# for i in range(len(url_map)):\n",
    "#     if url_map[\"central bank\"][i] == \"fed\" and url_map[\"document\"][i] == \"minutes\":\n",
    "#         minutes = pd.read_csv(url_map[\"finbert_url\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on datetime64[ns] and object columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m sent \u001b[38;5;241m=\u001b[39m sent[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     10\u001b[0m sent \u001b[38;5;241m=\u001b[39m sent\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: url_map[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]})\n\u001b[0;32m---> 11\u001b[0m sentiment \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/reshape/merge.py:148\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m--> 148\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/reshape/merge.py:741\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    733\u001b[0m (\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[1;32m    737\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merge_keys()\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/reshape/merge.py:1405\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m-> 1405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   1407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on datetime64[ns] and object columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "sentiment = pd.date_range(start=\"1/1/1990\", end=\"12/31/2024\", freq=\"D\")\n",
    "sentiment = pd.DataFrame(sentiment, columns=[\"date\"])\n",
    "\n",
    "url_map = pd.read_csv(os.path.join(cwd, \"url_map.csv\"))\n",
    "\n",
    "for i in range(len(url_map)):\n",
    "    sent = pd.read_csv(url_map[\"finbert_url\"][i])\n",
    "    sent = sent[[\"date\", \"sentiment\"]]\n",
    "\n",
    "    sent = sent.rename(columns={\"sentiment\": url_map[\"document\"][i]})\n",
    "    sentiment = pd.merge(sentiment, sent, how=\"outer\", left_on=\"date\", right_on=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns containing speech sentiment for all banks\n",
    "speech_sentiment_columns = [\n",
    "    'fed_speeches', \n",
    "    'ecb_speeches', \n",
    "    'boe_speeches', \n",
    "    'boj_speeches', \n",
    "    'boa_speeches', \n",
    "    'boc_speeches', \n",
    "    'boswiss_speeches', \n",
    "    'bosweden_speeches',\n",
    "]\n",
    "\n",
    "# Define the layout of the subplots\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(20, 20))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each sentiment column in its own subplot\n",
    "for i, column in enumerate(speech_sentiment_columns):\n",
    "    if column in sentiment.columns:\n",
    "        axes[i].plot(sentiment['date'], sentiment[column], label=column)\n",
    "        axes[i].set_xlabel('Date')\n",
    "        axes[i].set_ylabel('Speech Sentiment')\n",
    "        axes[i].set_title(column)\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = pd.read_csv(\n",
    "    \"/Users/kylenabors/Documents/Database/Market Data/SP500/SP500 Returns Daily.csv\"\n",
    ")\n",
    "\n",
    "sp500 = sp500.rename(columns={\"caldt\": \"date\", \"vwretd\": \"sp500_return\"})\n",
    "sp500[\"date\"] = pd.to_datetime(sp500[\"date\"])\n",
    "\n",
    "us_interest = pd.read_csv(\n",
    "    \"/Users/kylenabors/Documents/Database/Market Data/Fed Funds/Fed Funds.csv\"\n",
    ")\n",
    "us_interest[\"date\"] = pd.to_datetime(us_interest[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = sp500.rename(columns={\"caldt\": \"date\", \"vwretd\": \"sp500_return\"})\n",
    "sp500[\"date\"] = pd.to_datetime(sp500[\"date\"])\n",
    "minutes = minutes[[\"date\", \"sentiment\"]]\n",
    "minutes = minutes.rename(columns={\"sentiment\": \"minute_sentiment\"})\n",
    "minutes[\"date\"] = pd.to_datetime(minutes[\"date\"])\n",
    "minutes2 = minutes.copy(deep=True)\n",
    "minutes = minutes.groupby(\"date\").mean().reset_index()\n",
    "\n",
    "filter_df = minutes.copy(deep=True)\n",
    "filter_df = filter_df[[\"date\", \"minute_sentiment\"]]\n",
    "\n",
    "cycle, trend = sm.tsa.filters.hpfilter(\n",
    "    filter_df[\"minute_sentiment\"], 1600 * ((8 / 4) ** 4)\n",
    ")\n",
    "\n",
    "filter_df[\"minute_sentiment_cycle\"] = cycle\n",
    "filter_df[\"minute_sentiment_trend\"] = trend\n",
    "\n",
    "filter_df = filter_df[[\"date\", \"minute_sentiment_cycle\"]]\n",
    "minutes = minutes.drop(columns=[\"minute_sentiment\"])\n",
    "filter_df = filter_df.rename(columns={\"minute_sentiment_cycle\": \"minute_sentiment\"})\n",
    "minutes = pd.merge(minutes, filter_df, on=\"date\", how=\"left\")\n",
    "minutes = minutes.groupby(\"date\").mean().reset_index()\n",
    "\n",
    "event = range(len(minutes))\n",
    "minutes[\"event\"] = event\n",
    "minutes = pd.merge(minutes, sp500, how=\"outer\", on=\"date\")\n",
    "minutes = pd.merge(minutes, us_interest, how=\"outer\", on=\"date\")\n",
    "\n",
    "temp = minutes.copy(deep=True)\n",
    "temp = temp[[\"event\", \"sp500_return\"]]\n",
    "temp[\"log_returns\"] = np.log(temp[\"sp500_return\"] + 1)\n",
    "temp = temp[[\"event\", \"log_returns\"]]\n",
    "temp = temp.groupby(\"event\").sum().reset_index()\n",
    "minutes = minutes.groupby(minutes[\"event\"]).mean().reset_index()\n",
    "\n",
    "minutes = pd.merge(minutes, temp, how=\"left\", left_on=\"event\", right_on=\"event\")\n",
    "\n",
    "\n",
    "minutes2[\"dup_number\"] = minutes2.groupby([\"date\"]).cumcount() + 1\n",
    "pivot = pd.pivot_table(\n",
    "    minutes2,\n",
    "    index=\"dup_number\",\n",
    "    columns=\"date\",\n",
    "    values=\"minute_sentiment\",\n",
    "    fill_value=None,\n",
    ")\n",
    "\n",
    "window = 10\n",
    "pivot = pivot.iloc[:100]\n",
    "\n",
    "\n",
    "pivot[\"mean\"] = pivot.mean(axis=1)\n",
    "pivot[\"var\"] = pivot.var(axis=1)\n",
    "pivot[\"sd\"] = pivot[\"var\"] ** 0.5\n",
    "pivot[\"ci\"] = 1.96 * np.sqrt(pivot[\"sd\"] / len(pivot[\"sd\"]))\n",
    "\n",
    "pivot[\"rmean\"] = pivot[\"mean\"].rolling(window).mean()\n",
    "pivot[\"rvar\"] = pivot[\"mean\"].rolling(window).var()\n",
    "pivot[\"rsd\"] = pivot[\"rvar\"] ** 0.5\n",
    "# pivot[\"ci\"] = 1.96 * np.sqrt(pivot[\"rsd\"] / len(pivot[\"rsd\"]))\n",
    "\n",
    "\n",
    "pivot[\"count\"] = pivot.count(axis=1, numeric_only=True)\n",
    "\n",
    "# fig, axs = plt.subplots(2, 1, figsize=(20, 10))\n",
    "# axs[0].plot(pivot[\"mean\"], color=\"blue\", label=\"mean\")\n",
    "# axs[0].fill_between(\n",
    "#     pivot.index,\n",
    "#     pivot[\"mean\"] - pivot[\"ci\"],\n",
    "#     pivot[\"mean\"] + pivot[\"ci\"],\n",
    "#     color=\"b\",\n",
    "#     alpha=0.1,\n",
    "# )\n",
    "# axs[1].plot(pivot[\"count\"], color=\"red\", label=\"standard deviation\")\n",
    "\n",
    "# axs[0].set_title(f\"Mean Tone By Line Number: {window} line window\", fontsize=40)\n",
    "# axs[1].set_title(\n",
    "#     f\"Standard Deviation of Tone By Line Number: {window} line window\", fontsize=40\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "pivot.to_csv(\"/Users/kylenabors/Downloads/pivot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    event       date  minute_sentiment  sp500_return  fedfunds  log_returns\n",
      "16   16.0 2000-02-03          0.121391      0.012133      5.71     0.012060\n",
      "17   17.0 2000-03-23          0.122481      0.018145      6.04     0.017982\n",
      "18   18.0 2000-05-18          0.146237     -0.007502      6.49    -0.007530\n",
      "19   19.0 2000-06-29          0.152421     -0.008714      6.76    -0.008752\n",
      "20   20.0 2000-08-24         -0.077930      0.001704      6.56     0.001703\n"
     ]
    }
   ],
   "source": [
    "minutes = minutes[minutes[\"date\"].dt.year >= 2000]\n",
    "print(minutes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash import Dash, html, dcc\n",
    "import dash_bootstrap_components as dbc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
