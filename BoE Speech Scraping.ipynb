{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://www.bankofengland.co.uk\"\n",
    "\n",
    "# Sitemap URL for speeches\n",
    "sitemap_url = base_url + \"/sitemap/speeches\"\n",
    "\n",
    "# Directory to save speeches\n",
    "speeches_dir = \"/Users/kylenabors/Documents/Database/Training Data/boe/boe_speeches\"\n",
    "os.makedirs(speeches_dir, exist_ok=True)\n",
    "\n",
    "# Set up a requests session with retries\n",
    "session = requests.Session()\n",
    "retries = Retry(\n",
    "    total=5,  # Total number of retries\n",
    "    backoff_factor=0.5,  # A backoff factor to apply between attempts\n",
    "    status_forcelist=[500, 502, 503, 504],  # HTTP status codes to retry\n",
    "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"],  # Methods to retry\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "# Get the sitemap content\n",
    "try:\n",
    "    response = session.get(sitemap_url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching sitemap: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "# Function to parse date from string\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%d %B %Y\")\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Compile regex patterns\n",
    "pdf_pattern = re.compile(r\"\\.pdf$\")\n",
    "date_pattern = re.compile(r\"(\\d{1,2} \\w+ \\d{4})\")\n",
    "\n",
    "# Iterate over all speech links\n",
    "for link in soup.find_all(\"a\", href=True):\n",
    "    href = link[\"href\"]\n",
    "    if pdf_pattern.search(href):\n",
    "        # Construct full URL\n",
    "        if href.startswith(\"http\"):\n",
    "            pdf_url = href\n",
    "        else:\n",
    "            pdf_url = base_url + href\n",
    "\n",
    "        # Extract filename\n",
    "        filename = os.path.basename(href)\n",
    "\n",
    "        # Check if file already exists\n",
    "        file_path = os.path.join(speeches_dir, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Skipping {filename} (already downloaded)\")\n",
    "            continue\n",
    "\n",
    "        # Extract date from link text or filename\n",
    "        text = link.get_text(strip=True)\n",
    "        date_match = date_pattern.search(text)\n",
    "        if date_match:\n",
    "            date_str = date_match.group(1)\n",
    "            date_obj = parse_date(date_str)\n",
    "        else:\n",
    "            # Try to extract date from filename\n",
    "            date_match = date_pattern.search(filename)\n",
    "            if date_match:\n",
    "                date_str = date_match.group(1)\n",
    "                date_obj = parse_date(date_str)\n",
    "            else:\n",
    "                date_obj = None\n",
    "\n",
    "        # Filter speeches from the last 30 years\n",
    "        # Attempt to download the PDF with retries\n",
    "        for attempt in range(5):\n",
    "            try:\n",
    "                pdf_response = session.get(pdf_url, timeout=10)\n",
    "                pdf_response.raise_for_status()\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(pdf_response.content)\n",
    "                print(f\"Downloaded {filename}\")\n",
    "                break  # Break the retry loop if successful\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error downloading {filename}: {e}\")\n",
    "                if attempt < 4:\n",
    "                    wait_time = (attempt + 1) * 2  # Exponential backoff\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to download {filename} after multiple attempts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Use PyMuPDF for better text extraction\n",
    "import fitz\n",
    "\n",
    "# Directory where PDFs are stored\n",
    "speeches_dir = \"BoE_Speeches\"\n",
    "\n",
    "# List to store extracted data\n",
    "data = []\n",
    "\n",
    "# Regex patterns\n",
    "date_pattern = re.compile(r\"(\\d{1,2} \\w+ \\d{4})\")\n",
    "speaker_pattern = re.compile(r\"By ([A-Za-z ,\\.]+)\")\n",
    "\n",
    "# Iterate over PDFs\n",
    "for filename in os.listdir(speeches_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(speeches_dir, filename)\n",
    "        print(f\"Processing {filename}...\")\n",
    "        text_content = \"\"\n",
    "        try:\n",
    "            with fitz.open(pdf_path) as doc:\n",
    "                for page in doc:\n",
    "                    text_content += page.get_text()\n",
    "\n",
    "            # Extract date from text or filename\n",
    "            date_match = date_pattern.search(text_content)\n",
    "            if date_match:\n",
    "                date_str = date_match.group(1)\n",
    "                date_obj = datetime.strptime(date_str, \"%d %B %Y\")\n",
    "            else:\n",
    "                # Try to extract date from filename\n",
    "                date_match = date_pattern.search(filename)\n",
    "                if date_match:\n",
    "                    date_str = date_match.group(1)\n",
    "                    date_obj = datetime.strptime(date_str, \"%d %B %Y\")\n",
    "                else:\n",
    "                    print(f\"Date not found for {filename}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "            # Extract speaker\n",
    "            speaker_match = speaker_pattern.search(text_content)\n",
    "            if speaker_match:\n",
    "                speaker = speaker_match.group(1).strip()\n",
    "            else:\n",
    "                speaker = \"Unknown\"\n",
    "\n",
    "            # Append to data list\n",
    "            data.append(\n",
    "                {\n",
    "                    \"date\": date_obj.strftime(\"%Y-%m-%d\"),\n",
    "                    \"group\": speaker,\n",
    "                    \"segment\": text_content,\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"BoE_Speeches.csv\", index=False)\n",
    "print(\"CSV file has been created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
